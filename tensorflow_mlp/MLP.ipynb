{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b4f184cff719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('../data/train.csv')\n",
    "testdata = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features1 = ['TotalBsmtSF']\n",
    "\n",
    "# features2 --> error after 100k train steps, learn=0.001\n",
    "# for a (2,20sig,10sig,1) MLP\n",
    "# = 0.42\n",
    "features2 = ['TotalBsmtSF', '1stFlrSF']\n",
    "\n",
    "# features3 --> error after 100k train steps, learn=0.001\n",
    "# for a (3,20sig,10sig,1) MLP\n",
    "# = 0.31\n",
    "features3 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea']\n",
    "\n",
    "# features4 --> error after 100k train steps, learn=0.001\n",
    "# for a (4,20sig,10sig,1) MLP\n",
    "# = 0.24\n",
    "features4 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual']\n",
    "\n",
    "# features5 --> error after 100k train steps, learn=0.001\n",
    "# for a (5,20sig,10sig,1) MLP\n",
    "# = 0.22\n",
    "features5 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual', 'GarageArea']\n",
    "\n",
    "# features6 --> error after 100k train steps\n",
    "# for a (6,20sig,10sig,1)   MLP = 0.22 (learn=0.001)\n",
    "# for a (6,20relu,10relu,1) MLP = 0.22 (learn=0.001)\n",
    "# for a (6,20relu,10relu,1) MLP = 0.26 (learn=0.0001)\n",
    "# for a (6,40relu,30relu,10relu,1) MLP = 0.34 (learn=0.0001)\n",
    "# for a (6,20sig,1) MLP = 0.26 (learn=0.001)\n",
    "# for a (6,20id,10id,1)   MLP = 0.26 (learn=0.001)\n",
    "features6 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual', 'GarageArea', 'GarageCars']\n",
    "\n",
    "# set feature vector to use here!\n",
    "features = features6\n",
    "\n",
    "# Normalization factor for house sale prices\n",
    "# This is important, since all the input feature values\n",
    "# \"live\" in different intervals\n",
    "# E.g. SalePrice: 50000-400000\n",
    "#      TotalBsmtSF: 300-2000\n",
    "#      OverallQual: 1-10\n",
    "normalization_factor_per_feature = {\"TotalBsmtSF\": 0.001,\n",
    "                                    \"1stFlrSF\": 0.001,\n",
    "                                    \"GrLivArea\": 0.001,\n",
    "                                    \"OverallQual\": 0.1,\n",
    "                                    \"GarageArea\": 0.001,\n",
    "                                    \"GarageCars\": 0.1,\n",
    "                                    \"SalePrice\": 0.00001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train matrix has 0 values which are 'nan'!\n",
      "test matrix has 0 values which are 'nan'!\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(traindata, testdata):\n",
    "    train_matrix = traindata[\"SalePrice\"].values\n",
    "    train_row_nr = len(train_matrix)\n",
    "    train_matrix = train_matrix.reshape(train_row_nr,1)\n",
    "    train_matrix = train_matrix * normalization_factor_per_feature[\"SalePrice\"]\n",
    "    test_matrix = testdata[\"Id\"].values\n",
    "    test_row_nr = len(test_matrix)\n",
    "    test_matrix = test_matrix.reshape(test_row_nr,1)\n",
    "    \n",
    "    for column_name in features:\n",
    "        train_column = traindata[column_name].values.reshape(train_row_nr,1)\n",
    "        test_column = testdata[column_name].values.reshape(test_row_nr,1)\n",
    "        train_column = train_column * normalization_factor_per_feature[column_name]\n",
    "        test_column = test_column * normalization_factor_per_feature[column_name]\n",
    "        train_matrix = np.hstack((train_matrix, train_column))\n",
    "        test_matrix = np.hstack((test_matrix, test_column))\n",
    "        missing_data_items_train = np.count_nonzero(np.isnan(train_matrix))\n",
    "        missing_data_items_test = np.count_nonzero(np.isnan(test_matrix))\n",
    "        print(\"train matrix has\",missing_data_items_train, \"values which are 'nan'!\")\n",
    "        print(\"test matrix has\",missing_data_items_test, \"values which are 'nan'!\")\n",
    "        nan_values_train = np.isnan(train_matrix)\n",
    "        train_matrix[nan_values_train] = 0\n",
    "        nan_values_test = np.isnan(test_matrix)\n",
    "        test_matrix[nan_values_test] = 0\n",
    "        return train_matrix, test_matrix\n",
    "    \n",
    "train_matrix, test_matrix = prepare_data(traindata, testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = '../data/result_al.csv'\n",
    "\n",
    "NR_NEURONS_HIDDEN1 = 20\n",
    "NR_NEURONS_HIDDEN2 = 10\n",
    "NR_NEURONS_OUTPUT  = 1\n",
    "\n",
    "NR_TRAIN_STEPS = 100000\n",
    "LEARN_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup(inputs):\n",
    "    input_node = tf.placeholder(tf.float32, shape=(1,inputs), name=\"input_node\")\n",
    "    teacher_node = tf.placeholder(tf.float32, name=\"teacher_node\")\n",
    "    \n",
    "    rnd_mat1 = tf.random_normal([nr_inputs, NR_NEURONS_HIDDEN1])\n",
    "    rnd_mat2 = tf.random_normal([NR_NEURONS_HIDDEN1, NR_NEURONS_HIDDEN2])\n",
    "    rnd_mat3 = tf.random_normal([NR_NEURONS_HIDDEN2, NR_NEURONS_OUTPUT])\n",
    "    \n",
    "    weights = {\n",
    "        'h1': tf.Variable(rnd_mat1),\n",
    "        'h2': tf.Variable(rnd_mat2),\n",
    "        'out': tf.Variable(rnd_mat3)\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([NR_NEURONS_HIDDEN1])),\n",
    "        'b2': tf.Variable(tf.random_normal([NR_NEURONS_HIDDEN2])),\n",
    "        'out': tf.Variable(tf.random_normal([NR_NEURONS_OUTPUT]))\n",
    "    }\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(input_node, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.leaky_relu(layer_2)\n",
    "    \n",
    "    output_node = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    output_node = tf.reshape(output_node, [])\n",
    "    \n",
    "    create_var_init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    loss_node = tf.abs(teacher_node - output_node)\n",
    "    optimizer_node = tf.train.GradientDescentOptimizer(LEARN_RATE).minimize(loss_node)\n",
    "    \n",
    "    return [input_node, teacher_node, create_var_init_op, loss_node, optimizer_node, output_node, weights['h1'],weights['h2'],weights['out']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_error(sess, model, train_matrix):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
