{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('../data/train.csv')\n",
    "testdata = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features1 = ['TotalBsmtSF']\n",
    "\n",
    "# features2 --> error after 100k train steps, learn=0.001\n",
    "# for a (2,20sig,10sig,1) MLP\n",
    "# = 0.42\n",
    "features2 = ['TotalBsmtSF', '1stFlrSF']\n",
    "\n",
    "# features3 --> error after 100k train steps, learn=0.001\n",
    "# for a (3,20sig,10sig,1) MLP\n",
    "# = 0.31\n",
    "features3 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea']\n",
    "\n",
    "# features4 --> error after 100k train steps, learn=0.001\n",
    "# for a (4,20sig,10sig,1) MLP\n",
    "# = 0.24\n",
    "features4 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual']\n",
    "\n",
    "# features5 --> error after 100k train steps, learn=0.001\n",
    "# for a (5,20sig,10sig,1) MLP\n",
    "# = 0.22\n",
    "features5 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual', 'GarageArea']\n",
    "\n",
    "# features6 --> error after 100k train steps\n",
    "# for a (6,20sig,10sig,1)   MLP = 0.22 (learn=0.001)\n",
    "# for a (6,20relu,10relu,1) MLP = 0.22 (learn=0.001)\n",
    "# for a (6,20relu,10relu,1) MLP = 0.26 (learn=0.0001)\n",
    "# for a (6,40relu,30relu,10relu,1) MLP = 0.34 (learn=0.0001)\n",
    "# for a (6,20sig,1) MLP = 0.26 (learn=0.001)\n",
    "# for a (6,20id,10id,1)   MLP = 0.26 (learn=0.001)\n",
    "features6 = ['TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OverallQual', 'GarageArea', 'GarageCars']\n",
    "\n",
    "# set feature vector to use here!\n",
    "features = features6\n",
    "\n",
    "# Normalization factor for house sale prices\n",
    "# This is important, since all the input feature values\n",
    "# \"live\" in different intervals\n",
    "# E.g. SalePrice: 50000-400000\n",
    "#      TotalBsmtSF: 300-2000\n",
    "#      OverallQual: 1-10\n",
    "normalization_factor_per_feature = {\"TotalBsmtSF\": 0.001,\n",
    "                                    \"1stFlrSF\": 0.001,\n",
    "                                    \"GrLivArea\": 0.001,\n",
    "                                    \"OverallQual\": 0.1,\n",
    "                                    \"GarageArea\": 0.001,\n",
    "                                    \"GarageCars\": 0.1,\n",
    "                                    \"SalePrice\": 0.00001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(traindata, testdata):\n",
    "    train_matrix = traindata[\"SalePrice\"].values\n",
    "    train_row_nr = len(train_matrix)\n",
    "    train_matrix = train_matrix.reshape(train_row_nr,1)\n",
    "    train_matrix = train_matrix * normalization_factor_per_feature[\"SalePrice\"]\n",
    "    test_matrix = testdata[\"Id\"].values\n",
    "    test_row_nr = len(test_matrix)\n",
    "    test_matrix = test_matrix.reshape(test_row_nr,1)\n",
    "    \n",
    "    for column_name in features:\n",
    "        train_column = traindata[column_name].values.reshape(train_row_nr,1)\n",
    "        test_column = testdata[column_name].values.reshape(test_row_nr,1)\n",
    "        train_column = train_column * normalization_factor_per_feature[column_name]\n",
    "        test_column = test_column * normalization_factor_per_feature[column_name]\n",
    "        train_matrix = np.hstack((train_matrix, train_column))\n",
    "        test_matrix = np.hstack((test_matrix, test_column))\n",
    "        missing_data_items_train = np.count_nonzero(np.isnan(train_matrix))\n",
    "        missing_data_items_test = np.count_nonzero(np.isnan(test_matrix))\n",
    "        print(\"train matrix has\",missing_data_items_train, \"values which are 'nan'!\")\n",
    "        print(\"test matrix has\",missing_data_items_test, \"values which are 'nan'!\")\n",
    "        nan_values_train = np.isnan(train_matrix)\n",
    "        train_matrix[nan_values_train] = 0\n",
    "        nan_values_test = np.isnan(test_matrix)\n",
    "        test_matrix[nan_values_test] = 0\n",
    "        return train_matrix, test_matrix\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = '../data/result_al.csv'\n",
    "\n",
    "NR_NEURONS_HIDDEN1 = 20\n",
    "NR_NEURONS_HIDDEN2 = 10\n",
    "NR_NEURONS_OUTPUT  = 1\n",
    "\n",
    "NR_TRAIN_STEPS = 100000\n",
    "LEARN_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup(inputs):\n",
    "    input_node = tf.placeholder(tf.float32, shape=(1,inputs), name=\"input_node\")\n",
    "    teacher_node = tf.placeholder(tf.float32, name=\"teacher_node\")\n",
    "    \n",
    "    rnd_mat1 = tf.random_normal([inputs, NR_NEURONS_HIDDEN1])\n",
    "    rnd_mat2 = tf.random_normal([NR_NEURONS_HIDDEN1, NR_NEURONS_HIDDEN2])\n",
    "    rnd_mat3 = tf.random_normal([NR_NEURONS_HIDDEN2, NR_NEURONS_OUTPUT])\n",
    "    \n",
    "    weights = {\n",
    "        'h1': tf.Variable(rnd_mat1),\n",
    "        'h2': tf.Variable(rnd_mat2),\n",
    "        'out': tf.Variable(rnd_mat3)\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([NR_NEURONS_HIDDEN1])),\n",
    "        'b2': tf.Variable(tf.random_normal([NR_NEURONS_HIDDEN2])),\n",
    "        'out': tf.Variable(tf.random_normal([NR_NEURONS_OUTPUT]))\n",
    "    }\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(input_node, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.leaky_relu(layer_2)\n",
    "    \n",
    "    output_node = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    output_node = tf.reshape(output_node, [])\n",
    "    \n",
    "    create_var_init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    loss_node = tf.abs(teacher_node - output_node)\n",
    "    optimizer_node = tf.train.GradientDescentOptimizer(LEARN_RATE).minimize(loss_node)\n",
    "    \n",
    "    return [input_node, teacher_node, create_var_init_op, loss_node, optimizer_node, output_node, weights['h1'],weights['h2'],weights['out']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_error(sess, model, train_matrix):\n",
    "    input_node, teacher_node, var_init_node, loss_node, optimizer_node, output_node, weights_h1, weights_h2, weights_out = model\n",
    "    \n",
    "    nr_train_samples = train_matrix.shape[0]\n",
    "    nr_input_features = train_matrix.shape[1] - 1\n",
    "    sum_losses = 0.0\n",
    "    \n",
    "    for sample_row_nr in range(0, nr_train_samples):\n",
    "        input_matrix = train_matrix[sample_row_nr, 1:]\n",
    "        input_matrix = input_matrix.reshape(1, nr_input_features)\n",
    "        saleprice = train_matrix[sample_row_nr, 0]\n",
    "        predicted_saleprice, sample_loss = sess.run([output_node, loss_node], feed_dict={input_node: input_matrix, teacher_node: saleprice})\n",
    "        sum_losses += sample_loss\n",
    "        \n",
    "    avg_loss = sum_losses / nr_train_samples\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_matrix, nr_steps_to_train):\n",
    "    input_node, teacher_node, var_init_node, loss_node, optimizer_node, output_node, weights_h1, weights_h2, weights_out = model\n",
    "    \n",
    "    nr_train_samples = train_matrix.shape[0]\n",
    "    nr_input_features = train_matrix.shape[1] - 1\n",
    "    sess = tf.Session()\n",
    "    sess.run(var_init_node)\n",
    "    \n",
    "    for train_step in range(1, nr_steps_to_train+1):\n",
    "        rnd_row = np.random.randint(0, nr_train_samples)\n",
    "        input_matrix = train_matrix[rnd_row, 1:]\n",
    "        input_matrix = input_matrix.reshape(1, nr_input_features)\n",
    "        saleprice = train_matrix[rnd_row, 0]\n",
    "        actual_output, tacher_value, loss_value, _, w_h1, w_h2, w_out = sess.run([output_node, teacher_node, loss_node, optimizer_node, weights_h1, weights_h2, weights_out], feed_dict={input_node: input_matrix, teacher_node: saleprice})\n",
    "        if train_step % 1000 == 0:\n",
    "            avg_error = compute_avg_error(sess, model, train_matrix)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sale prices for Houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sale_prices(sess, model, test_matrix):\n",
    "    input_node, teacher_node, var_init_node, loss_node, optimizer_node, output_node, weights_h1, weights_h2, weights_out = model\n",
    "    nr_test_samples = test_matrix.shape[0]\n",
    "    nr_input_features = test_matrix.shape[1] - 1\n",
    "    \n",
    "    prediction_matrix = np.zeros(shape=(nr_test_samples,2))\n",
    "    for row_nr in range(0, nr_test_samples):\n",
    "        input_matrix = test_matrix[row_nr, 1:]\n",
    "        input_matrix = input_matrix.reshape(1, nr_input_features)\n",
    "        house_id = int(test_matrix[row_nr, 0])\n",
    "        predicted_saleprice = sess.run(output_node, feed_dict={input_node: input_matrix})\n",
    "        print(\"House with id \", house_id,\n",
    "              \"has feature input_matrix = \", input_matrix,\n",
    "              \"--> predicted sale price is \", predicted_saleprice * (1.0/normalization_factor_per_feature[\"SalePrice\"]))\n",
    "        prediction_matrix[row_nr][0] = house_id\n",
    "        prediction_matrix[row_nr][1] = predicted_saleprice * (1.0/normalization_factor_per_feature[\"SalePrice\"])\n",
    "        \n",
    "        prediction_dataframe = pd.DataFrame({'Id':prediction_matrix[:,0],'SalePrice':prediction_matrix[:,1]})\n",
    "        prediction_dataframe = prediction_dataframe.astype({\"Id\": int})\n",
    "        print(prediction_dataframe)\n",
    "        \n",
    "        predection_dataframe.to_csv(OUTPUT_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train matrix has 0 values which are 'nan'!\n",
      "test matrix has 0 values which are 'nan'!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'houseid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-5ebba61537c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnr_input_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNR_TRAIN_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredict_sale_prices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-e5021b1e8c8b>\u001b[0m in \u001b[0;36mpredict_sale_prices\u001b[1;34m(sess, model, test_matrix)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mhouse_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_nr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpredicted_saleprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_node\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_matrix\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         print(\"House with id \", houseid,\n\u001b[0m\u001b[0;32m     13\u001b[0m               \u001b[1;34m\"has feature input_matrix = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m               \"--> predicted sale price is \", predicted_saleprice * (1.0/normalization_factor_per_feature[\"SalePrice\"]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'houseid' is not defined"
     ]
    }
   ],
   "source": [
    "train_matrix, test_matrix = prepare_data(traindata, testdata)\n",
    "nr_input_features = train_matrix.shape[1] - 1\n",
    "model = setup(nr_input_features)\n",
    "sess = train_model(model, train_matrix, NR_TRAIN_STEPS)\n",
    "predict_sale_prices(sess, model, test_matrix)\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
